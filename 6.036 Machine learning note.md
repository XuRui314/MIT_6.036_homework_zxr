# 6.036 Machine learning

> We are destined to meet again.

<img src="https://s2.loli.net/2022/02/10/B7RJAwa2WTQnXgp.jpg" style="zoom: 50%;" />









## Overview or Big picture?

/loading 





## Some Background

### Information Theory

交叉熵(`cross entropy`)



https://www.bilibili.com/video/BV15V411W7VB?t=1094.5

https://zhuanlan.zhihu.com/p/165139520





### Loss function

MSE, Cross Entropy 和 Hinge loss 的比较 - akon的文章 - 知乎 https://zhuanlan.zhihu.com/p/158448293

常见的损失函数(loss function)总结 - yyHaker的文章 - 知乎 https://zhuanlan.zhihu.com/p/58883095



### Optimization

**SVM**

> basic knowledge

**Part 1 Understanding of Lagrange multipliers**



> Q: Since $h_i(x) = 0$, why do we still add it?

ANS: Because what we focus on is gradient, which may not be 0. 



 https://www.zhihu.com/question/58584814/answer/159863739



**Part 2 Convex Optimization**

the definition of convex set and convex optimization problem

https://www.bilibili.com/video/BV1HP4y1Y79e?t=1139.7



**Part 3 Understanding of Lagrange duality** 



[(42条消息) 对偶函数求解（一）_yu132563的专栏-CSDN博客_对偶函数](https://blog.csdn.net/yu132563/article/details/111402607)

https://www.cnblogs.com/90zeng/p/Lagrange_duality.html

 https://www.zhihu.com/question/300015357/answer/715354302



> Q: Why Dual?

ANS: Because of convex: https://blog.csdn.net/weixin_44273380/article/details/109034549

Get an intuitive understanding from the picture below:

<img src="https://s2.loli.net/2022/02/20/SVT42p1I3vfxXoO.png" style="zoom:80%;" />

strong duality v.s. weak duality

https://www.bilibili.com/video/BV1HP4y1Y79e?t=1565.5



**Part 4 Slater / KKT**

https://www.bilibili.com/video/BV1HP4y1Y79e?t=2215.5



**Part 5 soft margin and hard margin**

https://zhuanlan.zhihu.com/p/94214743



[支持向量机(SVM)解读 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/66789668?utm_source=wechat_session)

https://www.youtube.com/watch?v=6-ntMIaJpm0

https://www.bilibili.com/video/BV1HP4y1Y79e?spm_id_from=333.999.0.0

https://blog.csdn.net/qq_41995574/article/details/89676894

## Introduction

This note will focus on the topics that mit note doesn't cover so much, and some great ideas during the implement of HW.



## Chapter 4 : Feature representation



**The number of features** becomes a problem if there are so many of them that some data points could have been identified by just looking at very few of their coordinates. Here it is not the case.



这部分的HW真的很棒，不同data形式对于最后的accuracy的影响的尝试与分析。

首先是**Feature Transformations**，包括

1. **Scaling**
2. **Encoding Discrete Values**
3. **Polynomial Features**

其实也就是对应了课上讲的那几种处理raw data的方式的实操。

接下来是**Experiments**，结合之前所学的应用，非常棒，具体对应的数据集和解释在[lab3](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week3/week3_lab/?child=last)中有。 

第一个是**Evaluating algorithmic and feature choices for AUTO data**，这是最简单的形式，auto data给好了，然后接下来就是选择`raw`, `standard` and `one_hot`三种数据形式， `perceptron` and `average perceptron` 两种算法，以及其中参数`T`的值，最后评测算法`eval_classifier` and `xval_learning_alg`。

第二个实验是**Evaluating algorithmic and feature choices for review data**，这个是结合具体案例，是评论系统，也就是自然语言处理，用的是[bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) (BOW) approach，还有最后对most positive和negative的选择，很有意思。

最后一个实验是**Evaluating features for MNIST data**，也就是对应CV的数据集，涉及对于数据集压缩的处理。

其实进一步来说，个人觉得，load_data到最后处理data成向量的部分，虽然以及提供了implement，但是还是有必要去掌握实现的，后期应该会完成~



## Chapter 5: Margin Maximization(2018 version)

### Regularization

L1正则和L2正则



### SVM





## Chapter 5: Logistic regression(2019 version)

这里mit改了一下，其实感知机后面接SVM或者逻辑回归感觉都不错，(所以为啥不干脆一起讲了捏\*\~(￣▽￣)\~*)























